{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Reinforcement Learning\n",
    "\n",
    "##  l'Environnement grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des biblio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiale les variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 4\n",
    "height = 4\n",
    "num_states = width * height\n",
    "S = np.arange(num_states)\n",
    "A = np.arange(4)  # 0: left, 1: Right, 2: Up, 3: Down\n",
    "T = np.array([width - 1, num_states - 1])\n",
    "P = np.zeros((len(S), len(A), len(S), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in S:\n",
    "    if (s % width) == 0:\n",
    "        P[s, 0, s, 0] = 1.0\n",
    "    else:\n",
    "        P[s, 0, s - 1, 0] = 1.0\n",
    "    if (s + 1) % width == 0:\n",
    "        P[s, 1, s, 0] = 1.0\n",
    "    else:\n",
    "        P[s, 1, s + 1, 0] = 1.0\n",
    "    if s < width:\n",
    "        P[s, 2, s, 0] = 1.0\n",
    "    else:\n",
    "        P[s, 2, s - width, 0] = 1.0\n",
    "    if s >= (num_states - width):\n",
    "        P[s, 3, s, 0] = 1.0\n",
    "    else:\n",
    "        P[s, 3, s + width, 0] = 1.0\n",
    "\n",
    "P[width - 1, :, :, 0] = 0.0\n",
    "P[num_states - 1, :, :, 0] = 0.0\n",
    "\n",
    "P[:, :, width - 1, 1] = -5.0\n",
    "P[:, :, num_states - 1, 1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir les fonction d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset() -> int:\n",
    "    return 0\n",
    "\n",
    "\n",
    "def is_terminal(state: int) -> bool:\n",
    "    return state in T\n",
    "\n",
    "\n",
    "def step(state: int, action: int) -> (int, float, bool):\n",
    "    assert not is_terminal(state)\n",
    "    next_state = np.random.choice(S, p=P[state, action, :, 0])\n",
    "    reward = P[state, action, next_state, 1]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definire le policy randon uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_random_uniform_policy(state_size: int, action_size: int) -> np.ndarray:\n",
    "    assert action_size > 0\n",
    "    return np.ones((state_size, action_size,)) / action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(\n",
    "        S: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        Pi: np.ndarray,\n",
    "        gamma: float = 0.99,\n",
    "        theta: float = 0.00001\n",
    ") -> np.ndarray:\n",
    "    assert theta > 0\n",
    "    assert 0 <= gamma <= 1\n",
    "    V = np.random.random((S.shape[0],))\n",
    "    V[T] = 0.0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_temp = V[s]\n",
    "            new_v = 0\n",
    "            for a in A:\n",
    "                for s_p in S:\n",
    "                    new_v += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "            V[s] = new_v\n",
    "            delta = np.maximum(delta, np.abs(v_temp - new_v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation policy random :\n",
      "[-1.95551426 -2.28653885 -3.1369096   0.         -1.70353726 -1.85959763\n",
      " -2.20043799 -2.69685022 -1.36435555 -1.32302373 -1.19730831 -0.94857625\n",
      " -1.1216561  -0.92430044 -0.36557768  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation policy random :\")\n",
    "Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "V = iterative_policy_evaluation(S, A, P, T, Pi)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "        S: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        gamma: float = 0.99,\n",
    "        theta: float = 0.00001\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "    while True:\n",
    "        V = iterative_policy_evaluation(S, A, P, T, Pi, gamma, theta)\n",
    "        policy_stable = True\n",
    "        for s in S:\n",
    "            old_action = np.argmax(Pi[s])\n",
    "            best_action = 0\n",
    "            best_action_score = -9999999999\n",
    "            for a in A:\n",
    "                tmp_sum = 0\n",
    "                for s_p in S:\n",
    "                    tmp_sum += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "                if tmp_sum > best_action_score:\n",
    "                    best_action = a\n",
    "                    best_action_score = tmp_sum\n",
    "            Pi[s] = 0.0\n",
    "            Pi[s, best_action] = 1.0\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            break\n",
    "    V = iterative_policy_evaluation(S, A, P, T, Pi, gamma, theta)\n",
    "    return V, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95099005 0.96059601 0.970299   0.         0.96059601 0.970299\n",
      " 0.9801     0.99       0.970299   0.9801     0.99       1.\n",
      " 0.9801     0.99       1.         0.        ]\n",
      "[[0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "time : 0.16193222999572754\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "V, Pi = policy_iteration(S, A, P, T)\n",
    "print(V)\n",
    "print(Pi)\n",
    "print(f\"time : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    S: np.ndarray,\n",
    "    A: np.ndarray,\n",
    "    P: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 0.00001\n",
    ") -> np.ndarray:\n",
    "    V = np.random.random((S.shape[0],))\n",
    "    V[T] = 0.0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_temp = V[s]\n",
    "            v_max = -99999999\n",
    "            for a in A:\n",
    "                v_max_temp = 0\n",
    "                for s_p in S:\n",
    "                    v_max_temp += P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "                if(v_max < v_max_temp):\n",
    "                    v_max = v_max_temp\n",
    "            V[s] = v_max\n",
    "            delta = np.maximum(delta, np.abs(v_temp - v_max))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "    for s in S:\n",
    "        old_action = np.argmax(Pi[s])\n",
    "        best_action = 0\n",
    "        best_action_score = -9999999999\n",
    "        for a in A:\n",
    "            tmp_sum = 0\n",
    "            for s_p in S:\n",
    "                tmp_sum += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                        P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                )\n",
    "            if tmp_sum > best_action_score:\n",
    "                best_action = a\n",
    "                best_action_score = tmp_sum\n",
    "        Pi[s] = 0.0\n",
    "        Pi[s, best_action] = 1.0\n",
    "    return V, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95099005 0.96059601 0.970299   0.         0.96059601 0.970299\n",
      " 0.9801     0.99       0.970299   0.9801     0.99       1.\n",
      " 0.9801     0.99       1.         0.        ]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "le time d'execution : 0.011749744415283203\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "V, Pi = value_iteration(S, A, P, T)\n",
    "print(V)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir un utile pour simules des step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_until_the_end_of_the_episode_and_generate_trajectory(\n",
    "        s0: int,\n",
    "        pi: np.ndarray,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_steps: int = 10\n",
    ") -> ([int], [int], [int], [float]):\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    s_p_list = []\n",
    "    r_list = []\n",
    "    st = s0\n",
    "    actions = np.arange(pi.shape[1])\n",
    "    step = 0\n",
    "    while not is_terminal_func(st) and step < max_steps:\n",
    "        at = np.random.choice(actions, p=pi[st])\n",
    "        st_p, rt_p = step_func(st, at)\n",
    "        s_list.append(st)\n",
    "        a_list.append(at)\n",
    "        s_p_list.append(st_p)\n",
    "        r_list.append(rt_p)\n",
    "        st = st_p\n",
    "        step += 1\n",
    "\n",
    "    return s_list, a_list, s_p_list, r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms monte carlo es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "    actions = np.arange(actions_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, actions_count))\n",
    "    returns_count = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s0 = np.random.choice(states)\n",
    "\n",
    "        if is_terminal_func(s0):\n",
    "            episode_id -= 1\n",
    "            continue\n",
    "\n",
    "        a0 = np.random.choice(actions)\n",
    "\n",
    "        s1, r1 = step_func(s0, a0)\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s1, pi, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "        s_list.insert(0, s0)\n",
    "        a_list.insert(0, a0)\n",
    "        r_list.insert(0, r1)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            if (st, at) in zip(s_list[0:t], a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, at] += G\n",
    "            returns_count[st, at] += 1\n",
    "            Q[st, at] = returns[st, at] / returns_count[st, at]\n",
    "            pi[st, :] = 0.0\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms monte_carlo_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.92191482  0.94804383  0.92426197  0.93599526]\n",
      " [ 0.93145844  0.95168495  0.94316664  0.9602388 ]\n",
      " [ 0.94764964 -5.          0.95507534  0.96953826]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.94171607  0.95866071  0.93186564  0.94574414]\n",
      " [ 0.9423193   0.97010728  0.94262422  0.96695723]\n",
      " [ 0.95761182  0.97695486  0.95715985  0.9801    ]\n",
      " [ 0.96706397  0.9801     -5.          0.99      ]\n",
      " [ 0.95340921  0.970299    0.94129721  0.96307044]\n",
      " [ 0.96016024  0.97596308  0.9588069   0.9801    ]\n",
      " [ 0.97017522  0.99        0.96679044  0.98682418]\n",
      " [ 0.9801      0.99        0.9801      1.        ]\n",
      " [ 0.96369833  0.97997554  0.95938558  0.96882653]\n",
      " [ 0.970299    0.99        0.96984192  0.9801    ]\n",
      " [ 0.9801      1.          0.97672231  0.98603322]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "le time d'execution : 1.8435587882995605\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = monte_carlo_es(len(S), len(A), step, is_terminal,\n",
    "                        max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_monte_carlo(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "    actions = np.arange(actions_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, actions_count))\n",
    "    returns_count = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s0 = reset_func()\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, pi, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            if (st, at) in zip(s_list[0:t], a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, at] += G\n",
    "            returns_count[st, at] += 1\n",
    "            Q[st, at] = returns[st, at] / returns_count[st, at]\n",
    "            pi[st, :] = epsilon / actions_count\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9027372   0.87052012  0.90551516  0.94376221]\n",
      " [ 0.91864514  0.31641809 -0.30543843 -0.64418598]\n",
      " [-1.60866294 -5.          0.9060982   0.96664098]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.91384971  0.93640496  0.90541895  0.95480436]\n",
      " [ 0.23784166  0.94798949  0.5652377  -0.19208039]\n",
      " [ 0.95111707  0.66644711  0.22177151  0.97784569]\n",
      " [ 0.970299    0.51618868 -5.          0.98858925]\n",
      " [ 0.95420505  0.93965067  0.94322714  0.96595533]\n",
      " [ 0.93533893  0.61440631  0.67262906  0.97762651]\n",
      " [ 0.95869034  0.98904664  0.65978263  0.98548377]\n",
      " [ 0.97745364  0.99        0.9771695   1.        ]\n",
      " [ 0.96454444  0.97702392  0.95400809  0.96478263]\n",
      " [ 0.96524858  0.98838785  0.96334362  0.97639912]\n",
      " [ 0.97707622  1.          0.97761554  0.98813239]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "[[0.025 0.025 0.025 0.925]\n",
      " [0.925 0.025 0.025 0.025]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.    0.    0.    0.   ]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.925 0.025 0.025]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.925 0.025 0.025]\n",
      " [0.025 0.025 0.025 0.925]\n",
      " [0.025 0.925 0.025 0.025]\n",
      " [0.025 0.925 0.025 0.025]\n",
      " [0.025 0.925 0.025 0.025]\n",
      " [0.    0.    0.    0.   ]]\n",
      "le time d'execution : 3.4952499866485596\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = on_policy_first_visit_monte_carlo(len(S), len(A),\n",
    "                                            reset,\n",
    "                                            step,\n",
    "                                            is_terminal,\n",
    "                                            max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_control(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1,\n",
    "        epsilon_greedy_behaviour_policy: bool = False\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    b = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "        pi[s, :] = 0\n",
    "        pi[s, np.argmax(Q[s, :])] = 1.0\n",
    "\n",
    "    C = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        if epsilon_greedy_behaviour_policy:\n",
    "            for s in states:\n",
    "                b[s, :] = epsilon / actions_count\n",
    "                b[s, np.argmax(Q[s, :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "\n",
    "        s0 = reset_func()\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, b, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            C[st, at] += W\n",
    "\n",
    "            Q[st, at] += W / C[st, at] * (G - Q[st, at])\n",
    "            pi[st, :] = 0\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0\n",
    "            if np.argmax(Q[st, :]) != at:\n",
    "                break\n",
    "            W = W / b[st, at]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.93918721  0.95045966  0.94033228  0.94950645]\n",
      " [ 0.94102067  0.95974396  0.95075793  0.96048232]\n",
      " [ 0.95099005 -5.          0.96003349  0.970299  ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.95012847  0.9598703   0.94094413  0.95883345]\n",
      " [ 0.94895129  0.96931125  0.95006225  0.96924241]\n",
      " [ 0.95611422  0.97840139  0.95834988  0.9801    ]\n",
      " [ 0.96885725  0.9801     -5.          0.98886598]\n",
      " [ 0.9589312   0.96906654  0.94705938  0.96769766]\n",
      " [ 0.95802319  0.9798931   0.95827874  0.9786632 ]\n",
      " [ 0.96883403  0.99        0.96931125  0.98970009]\n",
      " [ 0.97949388  0.9875495   0.9794044   1.        ]\n",
      " [ 0.96849748  0.97766334  0.95706517  0.96925454]\n",
      " [ 0.96315695  0.98884265  0.96887974  0.97795536]\n",
      " [ 0.97702759  1.          0.9801      0.99      ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "le time d'execution : 12.472673654556274\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = off_policy_monte_carlo_control(len(S), len(A),\n",
    "                                           reset,\n",
    "                                           step,\n",
    "                                           is_terminal,\n",
    "                                           max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda3c04aedbaf13499cba3678afa7242089"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
