{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Reinforcement Learning\n",
    "\n",
    "##  l'Environnement TIC TAC TOE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des biblio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiale les variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = []\n",
    "A = np.arange(9)\n",
    "\n",
    "\n",
    "MY_COLOR = [0, 1, 0]\n",
    "ADVERSARY_COLOR = [0, 0, 1]\n",
    "EMPTY_CELL = [1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "def init_state(state = S, cell_list = [], cell_index = 0):\n",
    "    if(cell_index == 9):\n",
    "        state.append(cell_list)\n",
    "        return\n",
    "    for i in [[1, 0, 0], [0, 1, 0], [0, 0, 1]]:\n",
    "        init_state(state, copy(cell_list + [i]),cell_index + 1)\n",
    "    return\n",
    "\n",
    "init_state()\n",
    "S = np.asarray(S)\n",
    "\n",
    "\n",
    "s_start = np.where(S == np.array([[1, 0, 0] for i in range(9)]))[0][0]\n",
    "c_start = [0, 1, 0]\n",
    "\n",
    "print(S[s_start])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir les fonction d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available(state):\n",
    "    ret = []\n",
    "    state = S[state]\n",
    "    for i in range(len(state)):\n",
    "        if(np.array_equal(state[i], EMPTY_CELL)):\n",
    "            ret.append(i)\n",
    "    return np.array(ret)\n",
    "\n",
    "def step(state, action : int, color):\n",
    "    assert action >= 0 and action < 9, \"invalid\"\n",
    "    assert not is_terminal(state), \"game is finished\"\n",
    "    assert np.array_equal(S[state, action], EMPTY_CELL), \"cell is not empty\"\n",
    "    reward = 1\n",
    "    \n",
    "    state = np.copy(S[state])\n",
    "    state[action] = color\n",
    "    next_state = np.copy(state)\n",
    "    next_color = [[0, 1, 1][i] - color[i] for i in range(3)]\n",
    "    next_state_index = np.where(S == next_state)[0][0]\n",
    "    \n",
    "    if(len(get_available(next_state)) == 0):\n",
    "        reward = 1\n",
    "        return next_state_index, reward, next_color\n",
    "    elif(is_terminal_by(next_state, color)):\n",
    "        reward = 2\n",
    "        return next_state_index, reward, next_color\n",
    "    elif(is_terminal_by(next_state, next_color)):\n",
    "        reward = -1\n",
    "        return next_state_index, reward, next_color\n",
    "    \n",
    "    \n",
    "    \n",
    "    return next_state_index, reward, next_color\n",
    "\n",
    "\n",
    "\n",
    "def get_winner(state):\n",
    "    if(len(get_available(state)) == 0):\n",
    "        return 0\n",
    "    elif(is_terminal_by(state, MY_COLOR)):\n",
    "        return 1\n",
    "    elif(is_terminal_by(state, ADVERSARY_COLOR)):\n",
    "        return -1\n",
    "    \n",
    "def reset():\n",
    "    return s_start\n",
    "\n",
    "def is_terminal_by(state_index : int, color = MY_COLOR):\n",
    "    state = S[state_index]\n",
    "    return np.array([len(np.unique(state[[0,1,2]], axis=0)) == 1 and np.array_equal(state[0],color),\n",
    "    len(np.unique(state[[3, 4, 5]], axis=0)) == 1 and np.array_equal(state[3], color),\n",
    "    len(np.unique(state[[6, 7, 8]], axis=0)) == 1 and np.array_equal(state[6], color),\n",
    "    len(np.unique(state[[0, 3, 6]], axis=0)) == 1 and np.array_equal(state[0], color),\n",
    "    len(np.unique(state[[1, 4, 7]], axis=0)) == 1 and np.array_equal(state[1], color),\n",
    "    len(np.unique(state[[2, 5, 8]], axis=0)) == 1 and np.array_equal(state[2], color),\n",
    "    len(np.unique(state[[0, 4, 8]], axis=0)) == 1 and np.array_equal(state[0], color),\n",
    "    len(np.unique(state[[2, 4, 6]], axis=0)) == 1 and np.array_equal(state[2], color),]).any() or (len(get_available(state_index)) == 0)\n",
    "\n",
    "def is_terminal(state : int):\n",
    "    return is_terminal_by(state, MY_COLOR) or is_terminal_by(state, ADVERSARY_COLOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definire le policy random uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_random_uniform_policy(state_size: int, action_size: int) -> np.ndarray:\n",
    "    assert action_size > 0\n",
    "    return np.ones((state_size, 2, action_size)) / action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir un utile pour simules des step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_until_the_end_of_the_episode_and_generate_trajectory(\n",
    "        s0: int,\n",
    "        c0: int,\n",
    "        pi: np.ndarray,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_steps: int = 10\n",
    ") -> ([int], [int], [int], [float]):\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    r_list = []\n",
    "    c_list = []\n",
    "    st = s0\n",
    "    ct = c0\n",
    "    actions = np.arange(pi.shape[2])\n",
    "    step = 0\n",
    "    while not is_terminal_func(st) and step < max_steps:\n",
    "        at = np.random.choice(actions, p=pi[st, (ct.index(1) - 1)])\n",
    "        st_p, rt_p, ct_p = step_func(st, at, ct)\n",
    "        s_list.append(st)\n",
    "        a_list.append(at)\n",
    "        c_list.append(ct)\n",
    "        r_list.append(rt_p)\n",
    "        st = st_p\n",
    "        step += 1\n",
    "\n",
    "    return s_list, a_list, c_list, r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms monte carlo es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(\n",
    "        s0 : int,\n",
    "        c0 : np.ndarray,\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    \n",
    "    states = np.arange(states_count)\n",
    "    actions = get_available(s0)\n",
    "\n",
    "    Q = np.random.random((states_count, 2, actions_count))\n",
    "\n",
    "    for s in range(states_count):\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "            \n",
    "        availble_cells = get_available(s)\n",
    "        for c in range(2):\n",
    "            for a in range(actions_count):\n",
    "                if(a not in availble_cells):\n",
    "                    pi[s, c, a] = 0.0\n",
    "                    Q[s, c, a] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, 2, actions_count))\n",
    "    returns_count = np.zeros((states_count, 2, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s_temp = s0\n",
    "        \n",
    "        a0 = np.random.choice(actions)\n",
    "        \n",
    "        s1, r1, c1 = step_func(s_temp, a0, c0)\n",
    "\n",
    "        s_list, a_list, c_list , r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s1, c1, pi, step_func,\n",
    "                                                                                                  is_terminal_func,\n",
    "                                                                                                  max_steps_per_episode)\n",
    "        s_list.insert(0, s_temp)\n",
    "        a_list.insert(0, a0)\n",
    "        c_list.insert(0, c0)\n",
    "        r_list.insert(0, 1)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            ct = c_list[t]\n",
    "            # cr = [[0, 1, 1][i] - ct[i] for i in range(3)]\n",
    "            if (st, ct, at) in zip(s_list[0:t], c_list[0:t], a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, (ct.index(1) - 1), at] += G\n",
    "            returns_count[st, (ct.index(1) - 1), at] += 1\n",
    "            Q[st,(ct.index(1) - 1), at] = returns[st, (ct.index(1) - 1), at] / returns_count[st, (ct.index(1) - 1), at]\n",
    "            pi[st, (ct.index(1) - 1), :] = 0.0\n",
    "            pi[st, (ct.index(1) - 1), np.argmax(Q[st, (ct.index(1) - 1), :])] = 1.0\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms monte_carlo_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.46617457 10.46617457 10.46617457 10.46617457 10.46617457 10.46617457\n",
      " 10.46617457 10.46617457 10.46617457]\n",
      "[0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "le time d'execution : 96.46855568885803\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "s_start = np.array([[1,0,0],[0,1,0],[0,0,1],[0,1,0],[0,1,0],[0,0,1],[0,0,1],[1,0,0],[1,0,0]])\n",
    "s_start = np.where(S==s_start)[0][0]\n",
    "Q, Pi = monte_carlo_es(s_start, c_start, len(S), len(A), step, is_terminal,\n",
    "                                                      max_episodes=1000, max_steps_per_episode=10)\n",
    "\n",
    "print(Q[s_start, (c_start.index(1) - 1)])\n",
    "print(Pi[s_start, (c_start.index(1) - 1)])\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-449-25b8873ee143>, line 51)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-449-25b8873ee143>\"\u001b[1;36m, line \u001b[1;32m51\u001b[0m\n\u001b[1;33m    if (st, ct, at) in zip(s_list[0:t],c_list[0:t] a_list[0:t]):\u001b[0m\n\u001b[1;37m                                                        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def on_policy_first_visit_monte_carlo(\n",
    "        s0 : int,\n",
    "        c0 : np.ndarray,\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "    actions = np.arange(actions_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in range(states_count):\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "            \n",
    "        availble_cells = get_available(s)\n",
    "        for c in range(2):\n",
    "            for a in range(actions_count):\n",
    "                if(a not in availble_cells):\n",
    "                    pi[s, c, a] = 0.0\n",
    "                    Q[s, c, a] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, actions_count))\n",
    "    returns_count = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s0 = reset_func()\n",
    "        \n",
    "        s_list, a_list, c_list , r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s1, c1, pi, step_func,\n",
    "                                                                                                is_terminal_func,\n",
    "                                                                                                max_steps_per_episode)\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, pi, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            ct = c_list[t]\n",
    "            if (st, ct, at) in zip(s_list[0:t],c_list[0:t] a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, (ct.index(1) - 1),at] += G\n",
    "            returns_count[st, (ct.index(1) - 1),at] += 1\n",
    "            Q[st, (ct.index(1) - 1),at] = returns[st, (ct.index(1) - 1),at] / returns_count[st, (ct.index(1) - 1),at]\n",
    "            pi[st, (ct.index(1) - 1), :] = epsilon / actions_count\n",
    "            pi[st, (ct.index(1) - 1), np.argmax(Q[st, (ct.index(1) - 1), :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = on_policy_first_visit_monte_carlo(len(S), len(A),\n",
    "                                            reset,\n",
    "                                            step,\n",
    "                                            is_terminal,\n",
    "                                            max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_control(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1,\n",
    "        epsilon_greedy_behaviour_policy: bool = False\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    b = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "        pi[s, :] = 0\n",
    "        pi[s, np.argmax(Q[s, :])] = 1.0\n",
    "\n",
    "    C = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        if epsilon_greedy_behaviour_policy:\n",
    "            for s in states:\n",
    "                b[s, :] = epsilon / actions_count\n",
    "                b[s, np.argmax(Q[s, :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "\n",
    "        s0 = reset_func()\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, b, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            C[st, at] += W\n",
    "\n",
    "            Q[st, at] += W / C[st, at] * (G - Q[st, at])\n",
    "            pi[st, :] = 0\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0\n",
    "            if np.argmax(Q[st, :]) != at:\n",
    "                break\n",
    "            W = W / b[st, at]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = off_policy_monte_carlo_control(len(S), len(A),\n",
    "                                           reset,\n",
    "                                           step,\n",
    "                                           is_terminal,\n",
    "                                           max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda3c04aedbaf13499cba3678afa7242089"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
