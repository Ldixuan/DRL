{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Deep Reinforcement Learning\n",
    "\n",
    "##  l'Environnement Line World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import des biblio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initiale les variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_states = 5\n",
    "S = np.arange(num_states)\n",
    "A = np.array([0, 1])  # 0: left, 1 : right\n",
    "T = np.array([0, num_states - 1])\n",
    "P = np.zeros((len(S), len(A), len(S), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in S[1:-1]:\n",
    "    P[s, 0, s - 1, 0] = 1.0\n",
    "    P[s, 1, s + 1, 0] = 1.0\n",
    "P[1, 0, 0, 1] = -1.0\n",
    "P[num_states - 2, 1, num_states - 1, 1] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir les fonction d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset() -> int:\n",
    "    return num_states // 2\n",
    "\n",
    "\n",
    "def is_terminal(state: int) -> bool:\n",
    "    return state in T\n",
    "\n",
    "\n",
    "def step(state: int, action: int) -> (int, float):\n",
    "    assert not is_terminal(state)\n",
    "    next_state = np.random.choice(S, p=P[state, action, :, 0])\n",
    "    reward = P[state, action, next_state, 1]\n",
    "    return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definire le policy randon uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabular_random_uniform_policy(state_size: int, action_size: int) -> np.ndarray:\n",
    "    assert action_size > 0\n",
    "    return np.ones((state_size, action_size,)) / action_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_policy_evaluation(\n",
    "        S: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        Pi: np.ndarray,\n",
    "        gamma: float = 0.99,\n",
    "        theta: float = 0.00001\n",
    ") -> np.ndarray:\n",
    "    assert theta > 0\n",
    "    assert 0 <= gamma <= 1\n",
    "    V = np.random.random((S.shape[0],))\n",
    "    V[T] = 0.0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_temp = V[s]\n",
    "            new_v = 0\n",
    "            for a in A:\n",
    "                for s_p in S:\n",
    "                    new_v += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "            V[s] = new_v\n",
    "            delta = np.maximum(delta, np.abs(v_temp - new_v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms iterative policy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation policy random :\n",
      "[ 0.00000000e+00 -4.99994602e-01  5.34431323e-06  5.00002645e-01\n",
      "  0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation policy random :\")\n",
    "Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "V = iterative_policy_evaluation(S, A, P, T, Pi)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Policy \"Toujours vers la droite !\" :\n",
      "[0.     0.9801 0.99   1.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation Policy \"Toujours vers la droite !\" :')\n",
    "Pi = np.zeros((S.shape[0], A.shape[0]))\n",
    "Pi[1:-1, 1] = 1.0\n",
    "V = iterative_policy_evaluation(S, A, P, T, Pi)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Policy \"Toujours vers la gauche !\" :\n",
      "[ 0.     -1.     -0.99   -0.9801  0.    ]\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation Policy \"Toujours vers la gauche !\" :')\n",
    "Pi = np.zeros((S.shape[0], A.shape[0]))\n",
    "Pi[1:-1, 0] = 1.0\n",
    "V = iterative_policy_evaluation(S, A, P, T, Pi)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(\n",
    "        S: np.ndarray,\n",
    "        A: np.ndarray,\n",
    "        P: np.ndarray,\n",
    "        T: np.ndarray,\n",
    "        gamma: float = 0.99,\n",
    "        theta: float = 0.00001\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "    while True:\n",
    "        V = iterative_policy_evaluation(S, A, P, T, Pi, gamma, theta)\n",
    "        policy_stable = True\n",
    "        for s in S:\n",
    "            old_action = np.argmax(Pi[s])\n",
    "            best_action = 0\n",
    "            best_action_score = -9999999999\n",
    "            for a in A:\n",
    "                tmp_sum = 0\n",
    "                for s_p in S:\n",
    "                    tmp_sum += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "                if tmp_sum > best_action_score:\n",
    "                    best_action = a\n",
    "                    best_action_score = tmp_sum\n",
    "            Pi[s] = 0.0\n",
    "            Pi[s, best_action] = 1.0\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "        if policy_stable:\n",
    "            break\n",
    "    V = iterative_policy_evaluation(S, A, P, T, Pi, gamma, theta)\n",
    "    return V, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.9801 0.99   1.     0.    ]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "time : 0.003989219665527344\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "V, Pi = policy_iteration(S, A, P, T)\n",
    "print(V)\n",
    "print(Pi)\n",
    "print(f\"time : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(\n",
    "    S: np.ndarray,\n",
    "    A: np.ndarray,\n",
    "    P: np.ndarray,\n",
    "    T: np.ndarray,\n",
    "    gamma: float = 0.99,\n",
    "    theta: float = 0.00001\n",
    ") -> np.ndarray:\n",
    "    V = np.random.random((S.shape[0],))\n",
    "    V[T] = 0.0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in S:\n",
    "            v_temp = V[s]\n",
    "            v_max = -99999999\n",
    "            for a in A:\n",
    "                v_max_temp = 0\n",
    "                for s_p in S:\n",
    "                    v_max_temp += P[s, a, s_p, 0] * (\n",
    "                            P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                    )\n",
    "                if(v_max < v_max_temp):\n",
    "                    v_max = v_max_temp\n",
    "            V[s] = v_max\n",
    "            delta = np.maximum(delta, np.abs(v_temp - v_max))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    Pi = tabular_random_uniform_policy(S.shape[0], A.shape[0])\n",
    "    for s in S:\n",
    "        old_action = np.argmax(Pi[s])\n",
    "        best_action = 0\n",
    "        best_action_score = -9999999999\n",
    "        for a in A:\n",
    "            tmp_sum = 0\n",
    "            for s_p in S:\n",
    "                tmp_sum += Pi[s, a] * P[s, a, s_p, 0] * (\n",
    "                        P[s, a, s_p, 1] + gamma * V[s_p]\n",
    "                )\n",
    "            if tmp_sum > best_action_score:\n",
    "                best_action = a\n",
    "                best_action_score = tmp_sum\n",
    "        Pi[s] = 0.0\n",
    "        Pi[s, best_action] = 1.0\n",
    "    return V, Pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.     0.9801 0.99   1.     0.    ]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "le time d'execution : 0.0009968280792236328\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "V, Pi = value_iteration(S, A, P, T)\n",
    "print(V)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### definir un utile pour simules des step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_until_the_end_of_the_episode_and_generate_trajectory(\n",
    "        s0: int,\n",
    "        pi: np.ndarray,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_steps: int = 10\n",
    ") -> ([int], [int], [int], [float]):\n",
    "    s_list = []\n",
    "    a_list = []\n",
    "    s_p_list = []\n",
    "    r_list = []\n",
    "    st = s0\n",
    "    actions = np.arange(pi.shape[1])\n",
    "    step = 0\n",
    "    while not is_terminal_func(st) and step < max_steps:\n",
    "        at = np.random.choice(actions, p=pi[st])\n",
    "        st_p, rt_p = step_func(st, at)\n",
    "        s_list.append(st)\n",
    "        a_list.append(at)\n",
    "        s_p_list.append(st_p)\n",
    "        r_list.append(rt_p)\n",
    "        st = st_p\n",
    "        step += 1\n",
    "\n",
    "    return s_list, a_list, s_p_list, r_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms monte carlo es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_es(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "    actions = np.arange(actions_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, actions_count))\n",
    "    returns_count = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s0 = np.random.choice(states)\n",
    "\n",
    "        if is_terminal_func(s0):\n",
    "            episode_id -= 1\n",
    "            continue\n",
    "\n",
    "        a0 = np.random.choice(actions)\n",
    "\n",
    "        s1, r1 = step_func(s0, a0)\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s1, pi, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "        s_list.insert(0, s0)\n",
    "        a_list.insert(0, a0)\n",
    "        r_list.insert(0, r1)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            if (st, at) in zip(s_list[0:t], a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, at] += G\n",
    "            returns_count[st, at] += 1\n",
    "            Q[st, at] = returns[st, at] / returns_count[st, at]\n",
    "            pi[st, :] = 0.0\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms monte_carlo_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [-1.          0.98008031]\n",
      " [ 0.97027907  0.99      ]\n",
      " [ 0.9801      1.        ]\n",
      " [ 0.          0.        ]]\n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]]\n",
      "le time d'execution : 0.6852025985717773\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = monte_carlo_es(len(S), len(A), step, is_terminal,\n",
    "                                                      max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_policy_first_visit_monte_carlo(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "    actions = np.arange(actions_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "\n",
    "    returns = np.zeros((states_count, actions_count))\n",
    "    returns_count = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        s0 = reset_func()\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, pi, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            if (st, at) in zip(s_list[0:t], a_list[0:t]):\n",
    "                continue\n",
    "\n",
    "            returns[st, at] += G\n",
    "            returns_count[st, at] += 1\n",
    "            Q[st, at] = returns[st, at] / returns_count[st, at]\n",
    "            pi[st, :] = epsilon / actions_count\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms on policy first visit monte carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [-1.          0.97397235]\n",
      " [ 0.85655042  0.98861531]\n",
      " [ 0.97387833  1.        ]\n",
      " [ 0.          0.        ]]\n",
      "[[0.   0.  ]\n",
      " [0.05 0.95]\n",
      " [0.05 0.95]\n",
      " [0.05 0.95]\n",
      " [0.   0.  ]]\n",
      "le time d'execution : 1.1858654022216797\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = on_policy_first_visit_monte_carlo(len(S), len(A),\n",
    "                                            reset,\n",
    "                                            step,\n",
    "                                            is_terminal,\n",
    "                                            max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_policy_monte_carlo_control(\n",
    "        states_count: int,\n",
    "        actions_count: int,\n",
    "        reset_func: Callable,\n",
    "        step_func: Callable,\n",
    "        is_terminal_func: Callable,\n",
    "        max_episodes: int = 1000,\n",
    "        max_steps_per_episode: int = 10,\n",
    "        gamma: float = 0.99,\n",
    "        epsilon: float = 0.1,\n",
    "        epsilon_greedy_behaviour_policy: bool = False\n",
    ") -> (np.ndarray, np.ndarray):\n",
    "    b = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    pi = tabular_random_uniform_policy(states_count, actions_count)\n",
    "    states = np.arange(states_count)\n",
    "\n",
    "    Q = np.random.random((states_count, actions_count))\n",
    "\n",
    "    for s in states:\n",
    "        if is_terminal_func(s):\n",
    "            Q[s, :] = 0.0\n",
    "            pi[s, :] = 0.0\n",
    "        pi[s, :] = 0\n",
    "        pi[s, np.argmax(Q[s, :])] = 1.0\n",
    "\n",
    "    C = np.zeros((states_count, actions_count))\n",
    "\n",
    "    for episode_id in range(max_episodes):\n",
    "        if epsilon_greedy_behaviour_policy:\n",
    "            for s in states:\n",
    "                b[s, :] = epsilon / actions_count\n",
    "                b[s, np.argmax(Q[s, :])] = 1.0 - epsilon + epsilon / actions_count\n",
    "\n",
    "        s0 = reset_func()\n",
    "\n",
    "        s_list, a_list, _, r_list = step_until_the_end_of_the_episode_and_generate_trajectory(s0, b, step_func,\n",
    "                                                                                              is_terminal_func,\n",
    "                                                                                              max_steps_per_episode)\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1\n",
    "        for t in reversed(range(len(s_list))):\n",
    "            G = gamma * G + r_list[t]\n",
    "            st = s_list[t]\n",
    "            at = a_list[t]\n",
    "            C[st, at] += W\n",
    "\n",
    "            Q[st, at] += W / C[st, at] * (G - Q[st, at])\n",
    "            pi[st, :] = 0\n",
    "            pi[st, np.argmax(Q[st, :])] = 1.0\n",
    "            if np.argmax(Q[st, :]) != at:\n",
    "                break\n",
    "            W = W / b[st, at]\n",
    "    return Q, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test l'algorithms off policy monte carlo control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.        0.      ]\n",
      " [-1.        0.9801  ]\n",
      " [ 0.970299  0.99    ]\n",
      " [ 0.9801    1.      ]\n",
      " [ 0.        0.      ]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "le time d'execution : 2.143306016921997\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "Q, Pi = off_policy_monte_carlo_control(len(S), len(A),\n",
    "                                           reset,\n",
    "                                           step,\n",
    "                                           is_terminal,\n",
    "                                           max_episodes=10000, max_steps_per_episode=100)\n",
    "print(Q)\n",
    "print(Pi)\n",
    "print(f\"le time d'execution : {time.time() - t1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda3c04aedbaf13499cba3678afa7242089"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
